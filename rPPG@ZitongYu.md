Back to https://github.com/Kwangkee/rPPG
***

## Zitong Yu
- https://scholar.google.com/citations?hl=ko&user=ziHejLwAAAAJ&view_op=list_works&sortby=pubdate
- https://sites.google.com/view/zitongyu?pli=1

## Papers
- PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer, https://scholar.google.com/citations?view_op=view_citation&hl=ko&user=ziHejLwAAAAJ&sortby=pubdate&citation_for_view=ziHejLwAAAAJ:1sJd4Hv_s6UC

- PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer, https://scholar.google.com/citations?view_op=view_citation&hl=ko&user=ziHejLwAAAAJ&sortby=pubdate&citation_for_view=ziHejLwAAAAJ:2P1L_qKh6hAC

- Learning Motion-Robust Remote Photoplethysmography through Arbitrary Resolution Videos, 
-	The codes are available at https://github.com/LJW-GIT/Arbitrary_Resolution_rPPG. Main code of AAAI2023 paper. 
-	Motivation 도 훌륭하고, Zitong Yu 논문이고, 코드도 공개되었으니, 검토해 볼 만 하겠지요?
-	we propose two plug-and-play blocks (i.e., Physiological Signal Feature Extraction block (PFE) and Temporal Face Alignment block (TFA)) to capture resolution- and motion-robust rPPG features. “PFE/TFA 가 어떤 Backbone 과도 쉽게 결합할 수 있다”면 실용적인 의미도 클 수 있겠습니다.
-	To our best knowledge, there is still no solution proposed yet to counter the problem of rPPG measurement from arbitrary face resolution videos. 학습 시 뿐만 아니라 추론 시에도 임의 해상도를 대응할 수 있으면 장점이 될 듯


***
Back to the [Top](#)  
Back to https://github.com/Kwangkee/rPPG
